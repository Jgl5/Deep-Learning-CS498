
\documentclass[sigconf,authorversion]{acmart}
\usepackage{url}
\begin{document}
\title{Deep learning: Predicting Length of Stay using Healthcare Analytics II Kaggle dataset }
\author{Jesus Lopez}
\email{jgl5@hood.edu}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Hood College}
  \streetaddress{401 Rosemont Ave}
  \city{Frederick}
  \state{MD}
  \country{USA}
  \postcode{21701}
}
%
\begin{abstract}
  Of increasing importance in healthcare management in the United States is the efficient allocation of hospital resources in order to increase patient outcomes. One particular area of interest is accurately predicting patients' Length of Stay at a hospital; based on presumed Length of Stay estimates, healthcare providers may then make choices that may ultimately lead to decisions which increase positive care outcomes for patients, especially those who may be at risk of long hospitalization stays. This project explored whether a simple deep learning model could be made to make accurate predictions about the Length of Stay of a patient given some categorical features and data from Kaggle's Healthcare Analytics II data-set. Two models were developed for the purpose of the project; the first, engineered towards a multi-class classification prediction in terms of Length of Stay intervals, while the second was engineered towards a binary classification between an estimated Short or Long  Stay. Ultimately, both models resulted in low-accuracy predictions which showed high amounts of inaccuracy during validation. The results may be explained by fundamental flaws in the data-set that potentially included too few categories of sample data, which led to the model being unable to find any patterns of significance to use for making predictions of any utility to healthcare providers. Importantly, the  results of this experimental project could serve as a useful evaluation of the sort of feature engineering and/or curated data that would be needed to make much better models in the future.
\end{abstract}
%
%% The code below is generated by the tool at %% http://dl.acm.org/ccs.cfm
%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010405.10010444.10010446</concept_id>
       <concept_desc>Applied computing~Consumer health</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010405.10010444.10010449</concept_id>
       <concept_desc>Applied computing~Health informatics</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10002944.10011123.10011131</concept_id>
       <concept_desc>General and reference~Experimentation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}
\ccsdesc[300]{Applied computing~Consumer health}
\ccsdesc[300]{Applied computing~Health informatics}
\ccsdesc[500]{General and reference~Experimentation}
%
\keywords{healthcare analytics II dataset, length of stay, deep learning, binary classification, multiclass classification, categorical classification }
%
\maketitle
\section{Introduction}
The potential of deep learning models to find a place as aids to the management of healthcare in the United States is indisputable. With the rising costs of healthcare there is pressure on healthcare facilities in the United States to optimize the efficient use of resources to cut healthcare costs while maintaining or improving patient outcomes \cite{Hoyer}. The fact that deep learning is most suitable for finding patterns among large amounts of data and that healthcare records are often large and have countless pieces of information means that the domain of healthcare is ripe for the application and experimental use of deep learning. One area which deep learning would be a boon to healthcare managers would be in the prediction of patients' Length of Stay (LOS). Research shows a correlation between patient outcomes and patients' length of stay \cite{McDermott, Rojas} . Investigations by \cite{Grover} support the idea that healthcare workers make different decisions about patient care if they expect a patient will have longer stays. However, the ability to make predictions about patients' Length of Stay remain bound to the experience of the healthcare professional interacting with the patient \cite{Grover} and there is currently a lack of any definitive industry tool. However, the area is one which is receiving attention from groups focused on applied deep learning \cite{8791477}. Sharing in the pursuit, this project experimented with the use of some simple deep learning models to generate predictions of a high enough accuracy to potentially be useful in a clinical setting. Specifically, one simple model was four-layer sequential model that would accept the categorical data that included important pieces of information about a patient's severity of illness, their age, and other potentially useful categories which a model might discover a pattern to base predictions which would be outputted as a probability of eleven possible intervals of Length of Stay. The second model would be similar but would instead attempt to reduce the problem into a binary classification problem by generating a prediction that only stated whether a patient would be expected to have a hospital stay greater or less than ten days. This approach, will hopefully have more success as a similar approach, albeit with an entirely different and more sophisticated model has shown to have over 77 percent accuracy\cite{8791477}.
%
\section{Results}
 On training and validation the best achieved metrics, as shown in Figure \ref{fig: best_metric_model_one} was an accuracy of 29 percent, with all features, except the Admission Deposit feature taken as input. Meaning that in 29 percent of the times, model one achieved a probability distribution that accurately predicted the correct LOS interval. Effectively, this meant that even the best version of model one would not be able to solve the problem.
 For model two, the results were much higher than in model one, but still poor. Accuracy ranged from 81 percent 99 percent, but loss scores were even worse than in model one. Like in model there was an experimentation cycle of removing features, adding layers, changing input dimensions, and optimizer functions. However, many of the changes resulted in degrading accuracy and only moderate improvement to loss scores. In the end, the best results for model two, shown in Figure \ref{fig: best_metric_model_two}, was an accuracy of 100 percent, but with an atrocious loss score. As a result of the loss score, it was clear that model could not be considered practical for clinical settings. As shown by the visualization the results throughout the project, as shown on Figure \ref{fig: results-summary}, no model was able to satisfy the objectives of the project.
%
 \begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{best-results-model-categorical.png}
    \caption{best Results of model one}
    \label{fig: best_metric_model_one}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{best-binary-model-results.png}
    \caption{Best results of Model two}
    \label{fig: best_metric_model_two}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{results-asset.png}
    \caption{Shows moderate  illness admissions are strongly prevalent across LOS intervals }
    \label{fig: results-summary}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{summary_model_one.png}
    \caption{Model One Summary, Text Display}
    \label{fig:summary_output_model_one}
\end{figure}
%
\section{Discussion}
\subsection{Model design}
The data-set used, Healthcare Analytics II, found on Kaggle, has approximately 415,000 samples in total, with two comma-separated value (CSV) files splitting the total data-set into a training and testing set. The former had a column with samples for patient Length of Stay, while the test set did not have a column for Length of Stay.  Therefore, only the CSV file containing the 318,000 samples were used for the training of the two models used for this project. The data-set had a total of eighteen columns, of which one column was the target column (Length of Stay), one column, "case id" would be useless due to being unique. During the initial phase of Exploratory Data Analysis, there appeared to be some useful categories to serve as features, in particular the Severity of Illness category and Age ; even the rest of the categories seemed to have potential as features to better tune and train the models. In the initial rounds of model design, all the possibly useful features, shown in Figure \ref{fig: dataset cols} were at first utilized to train the first model, shown in Figure \ref{fig: code_model_one}, which is a sequential model built with Keras using four layers. At the final layer, an output would contain a probability of the most likely interval under which a patient's Length of Stay would fall under. In other words, because the data-set expressed Length of Stay in intervals, such as zero to ten days, eleven to twenty-one days, and so on, up until a period of more than one hundred days, the model would have to answer in terms of percentage; for example, the LOS for a patient would be 10 percent likely to be zero to ten days, 15 percent to be twenty to thirty-one days, and so on. The outcomes realized by model one was initially accuracy ranging from 17 to 20 percent, with stable, but high loss values. Subsequently, additional layers of different input dimensions and activation functions were tried in a trial-and-error fashion, but ultimately had worse accuracy, sometimes ranging as low as 10 percent. Then, different features were subtracted from the input. For example, the Admission Deposit, the City-Patient-Code, and Bed Grade, were removed because their potential could be ambiguous since the categorical values for them did not have a self-contained method for evaluating what was best. Specifically, the values for Bed Grade were given from 1.0 to 4.0 but the data did not express whether 1.0 stood for the best bed grade, the worst bed grade, or vice-versa. Therefore, from a feature engineering point, it seemed best to remove feature that might be adding confusion to the model's attempt to finder a useful pattern for generating better representations.
%
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{data-columns-summary.png}
    \caption{Data-set Columns}
    \label{fig: dataset cols}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{model_one_code.png}
    \caption{Model One Raw Code}
    \label{fig: code_model_one}
\end{figure}
%
\noindent Even before actual testing there was an expectation that the second model would be more fruitful since it would work under the theory that simplifying the problem space would generate better results. Specifically, by turning what was, with model one, a multi-class classification problem,into a binary classification problem by having the model only attempt to predict whether a patient would have a LOS greater than ten days or not. For the training of model two, the encoding of the Length of Stay did not use hot-one-encoding, as in model one, but instead used simple integer encoding where all the values in the Stay column were turned to zero if the categorical value corresponded to an interval of less than ten days, and an integer of one if the categorical value corresponded to an interval of more than ten days. This has the benefit of reducing the overall program space and allow for more resource efficient training epochs. For this model, shown in Figure \ref{fig: code_model_two} the same number of layers were applied at first, though different numbers of layers and dimensions were tried just as in the experimentation with model one. 
%
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{model_two_code.png}
    \caption{Model Two Raw Code}
    \label{fig: code_model_two}
\end{figure}
%
\subsection{Analysis of the data}
Given the  results, investigation of the underlying data seemed appropriate. Using the functionality of the Python programming language's Pandas library, visualizations for the data showed some potential flaws in the data-set itself. As shown by Figure \ref{fig: correlation-matrix} there seems to be no statistically significant correlation between the various categories which served as features for the model to train on. 
%
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{correlation-matrix.png}
    \caption{Matrix shows a lot of negative correlation and very little, if any, statistically significant correlation between the various categories}
    \label{fig: correlation-matrix}
\end{figure}
%
This was surprising given that one would expect at least a relationship between LOS and Severity of Illness, as shown in Figure \ref{fig: illness-to-LOS}. While this does not conclusively prove that the data-set's categories could not be used to generate a useful pattern - after all, the power of deep learning is the fact that the models can use mathematical operations to create abstract representations that a human may not be able to create themselves\cite{Abril07} -- it does seem that this data-set, despite being used for a Kaggle competition for the same task as this project, may not be ideal. 
%
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{dataset-illness-viz.png}
    \caption{Shows moderate  illness admissions are strongly prevalent across LOS intervals }
    \label{fig: illness-to-LOS}
\end{figure}
%
Further issues occur with the imbalance in the data. The representation between the admission categories, meaning the departments there are data for is very skewed toward Gynecology. Consequently, the danger of over-fitting was present and thus might explain the high accuracy score, but abysmal  loss score of model two. However, in model one, given the very poor accuracy, but better loss score, the opposite case, under-fitting, might be occurring if the model could not capture the proper relationship between the other admission types and the other features. 

\subsection{Future Work}
Any continuation of the work  with this data-set would either require far more extensive manipulation of the data-set, such as using technique of data augmentation, or entirely different models, such as a model such as used the proposed Autoencoder+DNN model used by Zebin et al. \cite{8791477} in their work on this very task, but using full medical records. In addition, an altogether new data-set might be worth serious consideration, such as MIMIC-III, which has been used by other deep learning groups \cite{johnson2016mimic}.
%
\begin{acks}
Very special thanks goes to fellow Hood College student, Ragi Ginu, a graduate student workings towards completion of a masters degree at the College. She was instrumental to this author's efforts to have working code for the multi-classification model. Without her efforts, the author would have spent countless more hours trying to code a solution to address an error where the data was not being properly accepted by the input layer of the model.
\end{acks}

\section*{Code Availability}
The source code including both training and testing of our networks, with pre-trained
model and demo data, are available ats
\url{https://github.com/Jgl5/Deep-Learning-Course}

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
\endinput
